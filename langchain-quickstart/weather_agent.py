"""
Real-World Weather Agent Example - LangChain Quickstart
Adapted for OpenAI instead of Anthropic
Demonstrates:
1. Detailed system prompts
2. Tools that integrate with external data
3. Model configuration
4. Structured output
5. Conversational memory
6. Full agent creation and execution
7. LangSmith tracing and observability
"""

import os
from dataclasses import dataclass

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.tools import tool, ToolRuntime
from langgraph.checkpoint.memory import InMemorySaver

# LangSmith tracing is automatically enabled when environment variables are set:
# - LANGSMITH_TRACING=true
# - LANGSMITH_API_KEY=<your-api-key>
# - LANGSMITH_PROJECT=<project-name> (optional but recommended)
# Get your API key from https://smith.langchain.com


# Define system prompt
SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""

# Define context schema
@dataclass
class Context:
    """Custom runtime context schema."""
    user_id: str


# Define tools
@tool
def get_weather_for_location(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"


@tool
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """Retrieve user information based on user ID."""
    user_id = runtime.context.user_id
    return "Florida" if user_id == "1" else "SF"


# Configure model - using OpenAI instead of Anthropic
model = init_chat_model(
    "gpt-4o-mini",  # Using OpenAI GPT-4o-mini
    temperature=0.5,
    timeout=10,
    max_tokens=1000
)


# Define response format
@dataclass
class ResponseFormat:
    """Response schema for the agent."""
    # A punny response (always required)
    punny_response: str
    # Any interesting information about the weather if available
    weather_conditions: str | None = None


# Set up memory
checkpointer = InMemorySaver()

# Create agent
agent = create_agent(
    model=model,
    system_prompt=SYSTEM_PROMPT,
    tools=[get_user_location, get_weather_for_location],
    context_schema=Context,
    response_format=ResponseFormat,
    checkpointer=checkpointer
)


# Run agent
if __name__ == "__main__":
    # Check if environment variables are set
    if not os.getenv("OPENAI_API_KEY"):
        print("Error: OPENAI_API_KEY environment variable is not set.")
        print("Please set: export OPENAI_API_KEY=<your-api-key>")
        exit(1)
    
    # Print LLM configuration
    print("=" * 50)
    print("ü§ñ LLM Configuration")
    print("=" * 50)
    print(f"Provider: OpenAI")
    print(f"Model: gpt-4o-mini")
    print(f"LLM Class: {type(model).__name__}")
    print(f"Temperature: 0.5")
    print(f"Max Tokens: 1000")
    print("=" * 50)
    
    # LangSmith tracing (optional but recommended)
    if os.getenv("LANGSMITH_TRACING") == "true":
        langsmith_project = os.getenv("LANGSMITH_PROJECT", "weather-agent-openai")
        print(f"‚úÖ LangSmith tracing enabled (project: {langsmith_project})")
        print(f"   View traces at: https://smith.langchain.com")
    else:
        print("‚ÑπÔ∏è  LangSmith tracing disabled. Set LANGSMITH_TRACING=true to enable.")
    
    # `thread_id` is a unique identifier for a given conversation.
    config = {"configurable": {"thread_id": "1"}}

    print("=== First Question ===")
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
        config=config,
        context=Context(user_id="1")
    )

    print(response['structured_response'])
    # Expected: ResponseFormat with punny_response and weather_conditions

    print("\n=== Follow-up Question ===")
    # Note that we can continue the conversation using the same `thread_id`.
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "thank you!"}]},
        config=config,
        context=Context(user_id="1")
    )

    print(response['structured_response'])
    # Expected: ResponseFormat with punny_response (weather_conditions may be None)

